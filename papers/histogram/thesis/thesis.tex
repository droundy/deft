\documentclass[11pt]{article}

%%% standard header
\usepackage[margin=1in]{geometry} % one inch margins
\usepackage{fancyhdr} % easier header and footer management
\pagestyle{fancyplain} % page formatting style
\usepackage{hyperref} % for linking references
\newcommand{\psl}{6pt} % store spacing as command for consistency
\setlength{\parindent}{0cm} % don't indent new paragraphs...
\parskip \psl % ... place a space between paragraphs instead
\usepackage[inline]{enumitem} % include for \setlist{}, use below
\frenchspacing % add a single space after a period
\usepackage{lastpage} % for referencing last page
\cfoot{\thepage~of \pageref{LastPage}} % "x of y" page labeling

%%% physics
\usepackage[boldvectors,braket]{physymb} % physics package
\newcommand{\bk}{\Braket} % shorthand for braket notation

%%% math
\usepackage{amsmath} % math package
\renewcommand{\t}{\text} % for text in math environment
\newcommand{\f}[2]{\dfrac{#1}{#2}} % shorthand for fractions
\newcommand{\p}[1]{\left(#1\right)} % parenthesis
\renewcommand{\sp}[1]{\left[#1\right]} % square parenthesis
\renewcommand{\set}[1]{\left\{#1\right\}} % curly parenthesis

%%% figures
\usepackage{graphicx,float,subcaption} % floats, etc.
\usepackage{footnote} % for footnotes in floats
\usepackage[font=small,labelfont=bf]{caption} % caption text options

%%% bibliography, table of contents
\usepackage[sort&compress,numbers]{natbib} % bibliography options
\bibliographystyle{apsrev4-1}
\usepackage[english]{babel} % allow editing bibliography title
\addto\captionsenglish{ % set bibliography title
  \renewcommand{\contentsname}{Table of Contents}
  \renewcommand\refname{\bf References}}
\usepackage{tocloft}
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}

%%% algorithm float
\floatstyle{plaintop}
\newfloat{algorithm}{thb}{lop}
\floatname{algorithm}{Algorithm}
\newenvironment{alg}
{\hrulefill\begin{enumerate}}
{\end{enumerate}\hrulefill}

\renewcommand{\title}{Optimizing Monte Carlo simulation of the
  square-well fluid} \renewcommand{\author}{Michael A. Perlin}
\newcommand{\institution}{Oregon State University}
\newcommand{\department}{Department of Physics}
\newcommand{\supervisor}{Dr. David Roundy}
\renewcommand{\date}{08 May 2015}

%%% notes, etc.
\usepackage{color}
\newcommand{\red}[1]{{\bf \color{red} #1}}
\newcommand{\fixme}[1]{[\red{fixme:} \emph{#1}]}

\fancypagestyle{abstract}{
  \rhead{}
  \lhead{}
}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{titlepage}

  \newcommand{\HRule}{\rule{\linewidth}{0.5mm}}

  \center

  \textsc{\LARGE \institution}\\[1.5cm]
  \textsc{\Large \department}\\[0.5cm]

  \HRule \\[0.4cm]
  { \huge \bfseries \title}\\[0.4cm]
  \HRule \\[1.5cm]

  \begin{minipage}{0.4\textwidth}
    \begin{flushleft} \large
      \emph{Author:}\\
      \author
    \end{flushleft}
  \end{minipage}
  ~
  \begin{minipage}{0.4\textwidth}
    \begin{flushright} \large
      \emph{Supervisor:} \\
      \supervisor
    \end{flushright}
  \end{minipage}\\[4cm]

  {\large\date}

  \vfill

\end{titlepage}

\thispagestyle{empty}

\newpage

\tableofcontents

\thispagestyle{empty}

\newpage

\listof{algorithm}{\Large List of Algorithms}

\listof{figure}{\Large List of Figures}

\thispagestyle{empty}

\newpage

\setcounter{page}{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Abstract}
\label{sec:abstract}

\thispagestyle{abstract}

We identify and develop efficient numerical methods for determining
thermodynamic properties of the square-well fluid in order to test
square-well density functional theories. The liquid-vapor phase
interface is an interesting regime for testing density functional
theories, but unbiased Metropolis Monte Carlo simulations are
incapable of sampling the low energy fluid states which dominate the
partition function at low (i.e. liquid-state) temperatures. Previous
works have developed several generic Monte Carlo histogram methods for
collecting statistics on low energy system states, but little or no
literature exists on these methods' systematic comparison, as well as
their application to the square-well fluid. The square-well fluid in
particular introduces application challenges not manifest in
traditional models for testing and benchmarking such numerical
techniques (e.g. the Ising model). We propose to implement these
methods in Monte Carlo simulations of the square-well fluid, determine
appropriate performance metrics for their comparison, and identify
those methods which are most efficient and effective at sampling
otherwise improbable system states.\fixme{make this an abstract,
  rather than a proposal...}

\fixme{Rewrite after the rest of the thesis is completed}

\fixme{Always talk about what we do in our own code in present, rather
  than past tense}


\newpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:intro}

\fixme{give more intro about phase transitions and the critical
  point?}

Understanding the behavior of fluids near the critical point, or the
point at which the distinction between liquid and gas ceases to become
well-defined, is a major challenge in the field of classical density
functional theory. Studying the critical point of a fluid via
simulations requires exploring the boundary between its liquid and
gaseous phases. The square-well fluid is the simplest system with a
liquid-vapor phase transition, and is therefore of interest for
studying the critical point of fluids. For a generic square-well fluid
to be observed in a liquid state, it must have a localized
distribution of the spheres which make up the fluid. Such a
distribution implies a low energy fluid state, as all interactions
between spheres are attractive. Conversely, a low energy liquid state
of a square-well fluid otherwise observable in both liquid and gaseous
phases implies a localized distribution of spheres. The search for
liquid states of the square-well fluid is thus equivalent to the
search for low energy states.

Monte Carlo simulations are a standard means for studying the
equilibrium thermodynamic properties of a system. In the most
straightforward implementation of Monte Carlo fluid simulations,
however, highly localized distributions of fluid elements are
extremely unlikely to occur, making such simulations impractical for
studying the critical point of the square-well fluid. Several generic
computational methods, called histogram methods, exist for dealing
with such a problem; namely, that interesting regions of a system's
state space cannot be sampled via standard Monte Carlo methods in a
reasonable amount of time. There is no common consensus, however, on
the relative efficacy of these methods. Furthermore, some methods,
such as the Wang-Landau method\cite{wang_landau}, require tuning
several free parameters that lack both ``obvious'' values and
systematic means for assigning them. This work will thus attempt to
quantify and compare the effectiveness of a few such methods as
applied to the square-well fluid.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background}
\label{sec:background}

\subsection{The square-well fluid}
\label{sec:sw_fluid}

\begin{figure}[tb]
  \centering
  \includegraphics[width=0.47\textwidth]{figs/square-well.pdf}
  \caption[The square-well pair potential]
  {The square-well pair potential can be used to model short range
    forces to first order. The infinite potential at $r<\sigma$ simply
    enforces the condition that spheres cannot overlap. At
    $r>\lambda\sigma$, the potential is null, and makes no
    contribution to the fluid's internal energy.}
  \label{fig:pair_potential}
\end{figure}

The square-well (SW) fluid is a simple model used in classical density
functional theories to capture low order effects of short-range
attractive forces, such as the van der Waals force. The fluid is
composed of spheres with diameter $\sigma$ which have a pair potential
\begin{align}
  v_{sw}\p{\vec r}=\left\{
    \begin{array}{ll}
      \infty & \abs{\vec r}<\sigma \\
      -\epsilon & \sigma<\abs{\vec r}<\lambda\sigma \\
      0 & \abs{\vec r}>\lambda\sigma
    \end{array}
  \right., \label{eq:pair_potential}
\end{align}
also shown graphically in Figure \ref{fig:pair_potential}, where the
parameters $\lambda$ and $\epsilon$ are referred to as the well width
and depth, respectively. The first ($\abs{\vec r}<\sigma$) part of
this potential forbids spheres from overlapping, whereas the second
($\sigma<\abs{\vec r}<\lambda\sigma$) associates an energy $-\epsilon$
with each pair of spheres whose centers are within distance of
$\lambda\sigma$ of each other (where typically, $\lambda\in(1,3]$).
The net potential energy of the square-well fluid is thus
\begin{align}
  E=\sum_{i<j}v_{sw}\p{\vec r_i-\vec r_j},
  \label{eq:internal_energy}
\end{align}
where $\vec r_i$ is the position of the $i$-th sphere. As the
potential energy $E$ is the primary form of energy concerning us in
this paper, we will refer to $E$ as simply the ``energy'' of the
fluid. An important feature of the square-well fluid is that its
energy is always an integer multiple of the well depth. A homogeneous
square-well fluid is thus uniquely identified by its well width
$\lambda$ and filling fraction $\eta$ (i.e. the proportion of space
filled by spheres; a dimensionless density), as all other properties
can be normalized to the natural energy scale $\epsilon$ and length
scale $\sigma$. In practice, our simulation codes use dimensionless
energies $E/\epsilon$, temperatures $kT/\epsilon$, and distances
$r/\sigma$.

\subsection{Monte Carlo fluid simulations}
\label{sec:mc_sim}

While model systems are powerful tools for understanding complex
physical systems, they do not themselves exist in the real
world. Consequently, direct experimental tests of theories for model
system (e.g. square-well density functional theories) are not
possible. For this reason, model system theories are commonly tested
against Monte Carlo simulations. Proper implementation of Monte Carlo
methods to study completely characterized systems ensures that
statistical results from simulations converge on the exact properties
of the simulated system in the infinite simulation time
limit. Furthermore, uncertainties in quantities computed via Monte
Carlo simulations are typically well-defined, monotonically decreasing
functions of simulation time, allowing one to run simulations to the
desired level of accuracy.

\subsubsection{Implementation}
\label{sec:mc_implementation}

\begin{algorithm}[tb]
  \caption{Metropolis Monte Carlo fluid simulation}
  \label{alg:metropolis}
  \begin{alg}

  \item Construct an initial ``typical'' (i.e. non-ordered) fluid
    configuration. \fixme{explain how}

  \item Randomly attempt to change the position of one sphere (in
    general, a single fluid ``atom'' or ``molecule'') to another
    location, rejecting the change if it results in a forbidden fluid
    configuration (e.g. two or more spheres overlap) and accepting the
    change otherwise. Attempting to move a single sphere is referred
    to as a move. \label{alg:metropolis_move}

  \item Repeat step \ref{alg:metropolis_move} for every other sphere
    in the fluid. Attempting to move every sphere once is referred to
    as an iteration of the simulation.
    \label{alg:metropolis_iteration}

  \item Repeat step \ref{alg:metropolis_iteration} indefinitely, or
    until data of sufficient quality has been generated, periodically
    collecting and dumping statistics on fluid states (e.g. energy,
    pair distribution histograms, etc.) to data files.

  \end{alg}
\end{algorithm}

Algorithm \ref{alg:metropolis} provides a sketch of unbiased,
Metropolis Monte Carlo fluid simulations. Such an algorithm is
``unbiased'' in the sense that it collects statistics (i.e. data) on
all valid system configurations with equal probability. Statistics
whose collection time scales as $\mathcal O\p{1}$ with system size,
meaning that increasing the number of spheres $N$ in the simulation
does not affect collection time, can be collected after every move
(defined in the algorithm). Statistics whose collection time scales as
$\mathcal O\p{N}$, meaning that doubling $N$ doubles collection time,
can be collected after each iteration. In general, collection with
$\mathcal O\sp{\chi\p{N}}$ time scaling should not occur more often
than once every $\chi\p{N}$ moves, where $\chi\p{N}$ may be an
arbitrary function of $N$ void of constant prefactors, e.g. $N\log N$,
or $2^N$. These collection rules ensure that scaling up simulations
does not cause them to asymptotically spend all computation time only
collecting statistics, or only simulating the fluid. Collected
statistics are used to find thermodynamic properties of the simulated
fluid.

In this work, we are concerned with simulating the \emph{homogeneous}
square-well fluid. To avoid edge effects resulting from fluid behavior
near a wall, we employ periodic boundary conditions. The use of a
finite cell with periodic boundary conditions suppresses all density
fluctuations on scales larger than the dimensions of the simulated
fluid cell, thereby introducing a source of error. Addressing and
sequestering this error, however, is outside the scope of this work,
and involves considering the limit of numerical results as the number
of spheres $N\to\infty$ (keeping the filling fraction $\eta$
constant).

\subsubsection{Computing observables}
\label{sec:computing_observables}

The sort of Monte Carlo simulations described above are only capable
of collecting statistics on functions of system microstates, and
finding correlations between these functions. For example, in
simulation one might periodically compute both the energy $E\p{s}$ and
some system property $X\p{s}$, both of which are determined by the
microstate $s$, in order to find the mean value of $X$ at any given
energy $E$, i.e. $\bk{X}_E$. In the real world, however, information
about a system's microstates, and by extension information about
functions of microstates (e.g. the energy $E$), is inaccessible. One
therefore cannot measure $\bk{X}_E$ directly. Instead, one typically
measures the dependence of thermodynamic properties on macroscopic
state variables such as temperature, i.e. $\bk{X}_T$.

To find $\bk{X}_T$, we first need to understand the concept of a
density of states. Given an arbitrary system property $Y\p{s}$
(e.g. energy) determined by the microstate $s$, one can define a
density of states $D\p{Y}$ such that
\begin{enumerate*}[label=\roman*)]
\item for arbitrary $Y_0$, the value of $D\p{Y_0}$ is proportional to
  the number of microstates $s$ for which $Y\p{s}=Y_0$, and
\item $\sum_YD\p{Y}=1$.
\end{enumerate*}
The units of $D\p{Y}$ are inverse to the units of $Y$. The temperature
dependence of $\bk{X}_T$ can be expressed in terms of the expectation
value $\bk{X}_E$ and the density of states $D\p{E}$ by
\begin{align}
  \bk{X}_T=\f1{Z\p{T}}\sum_E\bk{X}_ED\p{E}e^{-E/kT},
  \label{eq:XT_norm}
\end{align}
where the partition function $Z\p{T}$ is simply a normalization
factor, given by
\begin{align}
  Z\p{T}=\sum_ED\p{E}e^{-E/kT}.
\end{align}
To reduce redundant computations and numerical error in
implementation, we will generally use a partition function with an
unnormalized density of states $\tilde D\p{E}$, i.e.
\begin{align}
  \tilde Z\p{T}=\sum_E\tilde D\p{E}e^{-E/kT},
\end{align}
in terms of which
\begin{align}
  \bk{X}_T=\f1{\tilde Z\p{T}}\sum_E\bk{X}_E\tilde D\p{E}e^{-E/kT}.
  \label{eq:XT}
\end{align}

\subsection{Histogram methods}
\label{sec:histogram_methods}

Due to the fact that Metropolis Monte Carlo simulations sample all of
state space randomly and without preference, a histogram $H\p{X}$ of
observations of some system property $X$ is directly proportional to
the density of states $D\p{X}$; that is, the number of observations
$H\p{X_0}$ of the system with $X\p{s}=X_0$ is proportional to the
total number of states $s$ for which $X\p{s}=X_0$. It is sometimes the
case, however, that the density of states in some range $R$ of
possible $X$ is so low that it is practically impossible (via
Metropolis Monte Carlo) to sufficiently sample $R$, that is, to
accumulate a statistically significant histogram $H\p{X\in R}$, in any
reasonable amount of time. Given that the entropy $S\p{X}$ can be
expressed in terms of the number of microstates $\Omega\p{X}$ as
$S\p{X}=k\ln\Omega\p{X}\propto\ln D\p{X}$, we may refer to regions
with low state densities $D\p{X}$ as low entropy states.

Figure \ref{fig:dos_sample} provides an example of an unnormalized
density of states $\tilde D\p{E}$ in energy for a particular
square-well fluid. The density of states has been ``normalized'' to
have a maximum value of $1$ in order to emphasize the logarithmic span
of $D\p{E}$. This logarithmic span ($\sim10^{60}$) is proportional to
the number of moves which one must simulate via Metropolis Monte Carlo
in order to observe the energies with the lowest state densities. For
reference, the world's current fastest supercomputers can perform
$\sim10^{16}$ floating-point operations per second.

\begin{figure}
  \centering
  \includegraphics[width=0.6\textwidth]{figs/dos-example.pdf}
  \caption[Density of states]
  {Sample density of states for square-well fluid with a well width
    $\lambda=1.3$, filling fraction $\eta=0.3$, and $N=25$ spheres, as
    computed by several histogram methods. The density of states has
    been normalized to have a maximum value of $1$.}
  \label{fig:dos_sample}
\end{figure}

\subsubsection{Biased sampling}
\label{sec:biased_sampling}

Histogram methods provide a means to address Metropolis Monte Carlo's
inability to sufficiently sample low entropy states by introducing a
bias into the otherwise random sampling of state space. A weighting
function $w$ is introduced, whose domain is the value of some property
$X\p{s}$ which depends on the system state $s$. An additional
condition is then added to step \ref{alg:metropolis_move} of Algorithm
\ref{alg:metropolis} in order to accept an attempted move: the weights
$w\p{X_i}$ and $w\p{X_f}$ of the initial (pre-move) state $s_i$ and
final (post-move) state $s_f$, respectively, are used to determine the
probability $P_m\p{s_i\to s_f}$ of accepting an otherwise valid move
via
\begin{align}
  P_m\p{s_i\to s_f}=\min\set{\f{w\p{X_f}}{w\p{X_i}},1}.
  \label{eq:move_prob}
\end{align}
This formula means that when $w\p{X_f}>w\p{X_i}$, the move $s_i\to
s_f$ is accepted; when $w\p{X_f}<w\p{X_i}$, the ratio of these weights
determines the probability of accepting the move. Due to the fact that
only ratios of weights determine $P_m\p{s_i\to s_f}$, the weights
$w\p{X}$ are scale-invariant, meaning that their effect on simulations
is unchanged by scale factors that are constant with respect to
$X\p{s}$. Furthermore, simulating with flat weights $w\p{X}=w_0$ for
all $X$ is equivalent to simulating without weights, as the
probability $P_m\p{s_i\to s_f}$ will always equal 1.

Employing biased Monte Carlo simulations, sketched out in Algorithm
\ref{alg:biased_MC}, allows one to construct weight functions that
favor some region of state space over others, as transitions to states
with higher weights are always accepted, whereas transitions to states
with lower weights may be rejected, artificially preventing the
simulated system from leaving interesting regions of state
space. Crucially, the bias introduced by weights can be reversed when
computing system properties from sampling statistics, as the weight of
a particular state is directly proportional to the probability bias of
that state; that is, a state with a weight of 2 will be sampled twice
as often as it would have been with a weight of 1. If we wish to
convert a histogram $H\p{X}$ of observations in a biased Monte Carlo
simulation into a numerical unnormalized density of states $\tilde
D\p{X}$ in $X$, we therefore divide the histogram by the corresponding
weights $w\p{X}$, i.e.
\begin{align}
  \tilde D\p{X}=\f{H\p{X}}{w\p{X}}.
  \label{eq:dos}
\end{align}
The normalized density of states $D\p{X}$ in $X$ is then
\begin{align}
  D\p{X}=\f{\tilde D\p{X}}{\sum_X\tilde D\p{X}}
  =\f{H\p{X}/w\p{X}}{\sum_XH\p{X}/w\p{X}}.
  \label{eq:dos_norm}
\end{align}
In general, the domain and shape of the weight function will depend on
the desired yields (e.g.  density of states, heat capacity) of a
simulation. A histogram method is simply an algorithm or procedure for
determining a weight function appropriate for a particular simulation.

\begin{algorithm}[tb]
  \caption{Biased Monte Carlo fluid simulation}
  \label{alg:biased_MC}
  \begin{alg}

  \item Construct an appropriate weight function $w\sp{X\p{s}}$, whose
    argument is a system property $X\p{s}$ determined by the
    microstate $s$.

  \item Construct an initial typical fluid configuration.

  \item Randomly attempt to change the position of one sphere,
    rejecting the change if
    \begin{enumerate*}[label=\roman*)]
    \item it results in a forbidden fluid configuration, or
    \item a newly chosen random number on the interval $\sp{0,1}$ is
      larger than the probability determined by (\ref{eq:move_prob})
      for the initial and final states $s_i$ and $s_f$, respectively.
    \end{enumerate*}
    \label{alg:biased_mc_move}

  \item Repeat step \ref{alg:biased_mc_move} the simulation produces
    statistics of sufficient quality.

  \end{alg}
\end{algorithm}

In this paper, we will use a histogram $H\p{E}$, weights $w\p{E}$, and
density of states $D\p{E}$ which are functions of the square-well
fluid's energy $E$. Due to the fact that the square-well fluid can
only have discrete energies $E=-n\epsilon$, where $n$ is a
non-negative integer and $\epsilon$ is the well depth, in simulation
we store the weight function as an array of values. We will therefore
generally refer to the weight function as a ``weight array,'' or
simply ``weights.''

\subsubsection{Canonical weights}
\label{sec:canonical_weights}

The most common weight array $w\p{E}$ used by physicists for what is
called a canonical Monte Carlo simulation involves choosing a
particular temperature $T_0$, and using weights proportional to the
Boltzmann factor at that temperature, i.e.
\begin{align}
  w\p{E}=e^{-E/kT_0},
\end{align}
where there is no reason to normalize $w\p{E}$ due to the fact that
only ratios of weights, as per (\ref{eq:move_prob}), are ever used in
simulation. The fact that only ratios of weights are used in
simulation makes $w\p{E}$ scale-invariant.

The partition function at $T=T_0$ for simulations with canonical
weights is
\begin{align}
  \tilde Z\p{T_0}=\sum_E\tilde D\p{E}e^{-E/kT_0}
  =\sum_E\f{H\p{E}}{w\p{E}}~e^{-E/kT_0}
  =\sum_E\f{H\p{E}}{e^{-E/kT_0}}~e^{-E/kT_0}=\sum_EH\p{E},
  \label{Z_canonical}
\end{align}
and the value of a thermodynamic property $X\p{T_0}$
\begin{align}
  X\p{T_0}=\f1{\tilde Z\p{T_0}}\sum_E\bk{X}_E\tilde D\p{E}e^{-E/kT_0}
  =\f{\sum_E\bk{X}_EH\p{E}}{\sum_EH\p{E}},
  \label{eq:X_canonical}
\end{align}
where $\bk{X}_E$ is the mean value of $X$ at the energy $E$. The
simplifications in in (\ref{Z_canonical}) and (\ref{eq:X_canonical}),
which have no explicit dependence on $T_0$, occur because canonical
Monte Carlo simulations sample energies in proportion to the
distribution (over energy) of microstates at a temperature of $T_0$.
Canonical Monte Carlo simulations can therefore fail to sufficiently
sample energies which are important (i.e. energies with a
non-negligible state probability density) at different
temperatures. As a consequence, such simulations should not be used to
determine properties $X\p{T\ne T_0}$, and are thus referred to as
``fixed temperature'' simulations. Incidentally, Metropolis Monte
Carlo simulations can be thought of as infinite-temperature
simulations, as they are equivalent to simulations with canonical
weights
\begin{align}
  w\p{E}=\lim_{T\to\infty}e^{-E/kT}=1.
  \label{eq:inf_temp_weights}
\end{align}
This observation will be used in Section \ref{sec:min_energy} to
identify a maximum energy of interest.

Though canonical Monte Carlo is simple to implement, its inability to
investigate a system at more than one temperature at a time is a
disadvantage for determining the temperature dependence of system
properties. In order to find the behavior of $\bk{X}_T$, one must run
many simulations at discrete temperature intervals; each such
simulation will yield one data point on $\bk{X}_T$.

Sampling low energies, however, or understanding system behavior at
low temperatures, is even more problematic with canonical
weights. Using low temperature canonical weights will indeed force a
simulated system down to low energies, but will also likely freeze the
system into a local minimum of its energy landscape. Freezing into a
state in this manner will limit a simulation to sampling only a small
portion of the energy landscape, even though there may (and generally
will) be many other states with the same energy.

Due to these problems, we will not use canonical weights alone to
study the square-well fluid. We will, however, use canonical weights
for portions of all weight arrays, which we discuss in Section
\ref{sec:min_energy}.

\subsubsection{Broad energy sampling}
\label{sec:broad_energy_sampling}

Given the expression for $\bk{X}_T$ in (\ref{eq:XT}), sufficient
accumulation of statistics on $\bk{X}_E$ at all available energies in
principle allows one to determine $\bk{X}_T$ for any temperature
$T$. In practice, the density of states can fall off so quickly with
energy that some range of allowable energies is practically
inaccessible via Monte Carlo simulations, biased or otherwise. In such
a case, computing $\bk{X}_T$ to a reasonable degree of accuracy
requires sufficiently sampling the energies at which $\tilde
D\p{E}e^{-E/kT}$ dominates the sum in (\ref{eq:XT}). Section
\ref{sec:min_energy} discusses identifying the energies which are
important for computing system properties at a given temperature.

We will employ histogram methods in order to sample as broad of an
energy range as possible in each simulation. Broad energy sampling
will allow us to determine, for various square-well fluids,
\begin{enumerate*}[label=\roman*)]
\item the density of states $D\p{E}$, and
\item the temperature dependence $\bk{X}_T$ of various thermodynamic
  properties $X$, particularly at temperatures near the liquid-gas
  phase boundary.
\end{enumerate*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methods}
\label{sec:methods}

Having covered the basics of the square-well fluid as well as
Metropolis and biased Monte Carlo simulations, we can now discuss
several histogram methods for constructing energy weights. In Section
\ref{sec:flat_histogram}, we will introduce the flat histogram method,
which is not directly applicable to the study of most systems, but
which forms the basis of many other methods. We will then introduce,
in Section \ref{sec:simple_flat}, the ``simple flat'' method, which we
designed to be the simplest viable histogram method, and against which
we can compare more complicated methods. Next, in Sections
\ref{sec:wang_landau}--\ref{sec:optimized_ensemble} we introduce three
published methods: Wang-Landau, transition matrix Monte Carlo, and the
optimized ensemble. The last of these methods we will not actually
implement and analyze because, unlike most histogram methods, it was
designed to optimize weights rather than construct them. We will,
however, borrow ideas from the optimized ensemble to design a
histogram method of our own, a hybrid of the optimized ensemble and
transition matrix Monte Carlo, in Section \ref{sec:oetmmc}. Finally,
we will cover some general implementation details and loose ends in
Section \ref{sec:loose_ends}

\subsection{The flat histogram (multi-canonical) method}
\label{sec:flat_histogram}

The flat histogram method, also called the multi-canonical method,
assumes complete knowledge of the density of states $D\p{E}$ of the
system in question, and solves (\ref{eq:dos}) for a weight array
$w\p{E}$ which should yield a flat energy histogram $H\p E=H_0$ to get
\begin{align}
  w\p E=\f1{D\p E},
  \label{eq:flat_weights}
\end{align}
where we may neglect the constant scale factor $H_0$ and do not worry
about normalization factor distinguishing $D\p{E}$ from $\tilde
D\p{E}$ due to the scale-invariance of $w\p{E}$.

For all but the most trivial or well-studied systems, however, the
density of states $D\p{E}$ is not known prior to simulation, and is in
fact one of the system properties which a simulation is intended to
determine. Nonetheless, the this method has pedagogical value by
enabling discussion of an ``optimal'' weight array for generating a
flat energy histogram in simulations. All algorithms to construct
$w\p{E}$ with the aim of producing a flat energy histogram should
converge on the same weight array, given by (\ref{eq:flat_weights}).

\begin{algorithm}[b]
  \caption{A naive flat histogram method}
  \label{alg:flat_histogram}
  \begin{alg}

  \item Construct an initial typical fluid configuration.

  \item Simulate (without weights) for some time $\tau$, which may be
    real time, computer time, or a number of simulation iterations,
    collecting a histogram $H\p{E}$ of energy observations after every
    move.

  \item Set weights $w\p{E}=1/H\p{E}$. At energies with $H\p{E}=0$,
    set $w\p{E}=1$.

  \end{alg}
\end{algorithm}

Algorithm \ref{alg:flat_histogram} provides a naive way to implement
the flat histogram method without knowing a priori the density of
states $D\p{E}$. This algorithm involves first running a simulation
without weights (or with constant weights $w\p{E}=1$) while collecting
a histogram $H\p{E}$ of the observed energies $E$. After some time
(loosely defined), the histogram $H\p{E}$ may be taken as an
approximation of the density of states $\tilde D\p{E}$, and used via
(\ref{eq:flat_weights}) to generate a weight array for an actual
simulation. Algorithm \ref{alg:flat_histogram} has one free parameter,
which determines the amount of time for which to collect an energy
histogram $H\p{E}$.

The problem with this implementation of the flat histogram method is
that histogram methods are generally necessary when some range of
energies $R$ is inaccessible via Metropolis Monte Carlo
simulations. In such a case, the energy histogram $H\p{E\in R}$ after
simulating for any reasonable amount of time will be statistically
insignificant or null, invalidating the proportionality $H\p{E}\propto
D\p{E}$. This method will therefore only ``flatten'' the histogram at
energies $E$ which have been sufficiently sampled for $H\p{E}$ to be
statistically significant. The end goal in implementing histogram
methods, however, is precisely to sample those energies that cannot be
sufficiently sampled without employing clever schemes to bias Monte
Carlo simulations. Therefore, this method does not directly help us
study the square-well fluid. This method does, however, motivate the
method in Section \ref{sec:simple_flat}, and provides some of the
theory behind the methods in Sections \ref{sec:tmmc} and
\ref{sec:oetmmc}.

\subsection{The simple flat method}
\label{sec:simple_flat}

The ``simple flat'' method, developed by ourselves, extends the
implementation of the flat histogram method discussed in Section
\ref{sec:flat_histogram} to an algorithm intended to be the simplest
viable broad energy histogram method. While we do not expect this
method to outperform other, more sophisticated methods, the simple
flat method should provide a lower bar and a standard of comparison
for the performance of other methods. Any nontrivial histogram method
should, at a minimum, consistently outperform the simple flat method
in order to be considered for any applications.

The idea behind the simple flat method is such: after simulating for
some time, the energy histogram $H\p{E}$ will have a peak, or some
other uneven distribution of observations, in a region of energies $R$
for which $H\p{E\in R}$ statistically significant. One can thus use
the existing histogram to construct weights which should flatten
$H\p{R}$. Subsequent simulations should then spend less time at
energies with the highest state densities, and more time at energies
with relatively low state densities. The energy histogram from such
simulations should therefore be statistically significant in a larger
range of energies, improving the available estimate for a density of
states with which to compute flat histogram weights. One could then
use the improved estimate to construct new weights, and repeat this
simulation and updating process until simulations satisfy some end
condition (in fact, most histogram methods will require end
conditions, which is discussed in Section \ref{sec:end_conditions}).

\begin{algorithm}[tb]
  \caption{The simple flat method}
  \label{alg:simple_flat}
  \begin{alg}

  \item Construct an initial typical fluid configuration.

  \item Set $k=0$, choose an initial number of iterations $n$ for
    which to simulate, and initialize weights $w\p{E}=1$ for all $E$.

  \item Simulate for $n$ iterations (with weights), collecting a
    histogram $H\p{E}$ of energy observations after every move.
    \label{alg:simple_flat_sim}

  \item If some predetermined initialization end condition has
    \emph{not} been met, then
    \begin{enumerate}
    \item increment $k\leftarrow k+1$
    \item set $n_k=un_{k-1}$ with a predetermined factor $u\ge1$,
      \label{alg:simple_flat_f}
    \item update weights as $w\p{E}\leftarrow w\p{E}/H\p{E}$ for all
      $E$ at which $H\p{E}\ne0$,
      \label{alg:simple_flat_weight_update}
    \item return to step \ref{alg:simple_flat_sim}.
    \end{enumerate}

  \end{alg}
\end{algorithm}

Algorithm \ref{alg:simple_flat} provides an implementation of the
simple flat method. This algorithm has two free parameters: the
initial number of iterations $n_0$ for which to simulate and the
factor $u$. Due to the exponential growth of $n_k=u^kn_0$, this
algorithm should not be sensitive to the value of $n_0$.

The most interesting part of Algorithm \ref{alg:simple_flat} is step
\ref{alg:simple_flat_weight_update}, which updates the weights as
$w\p{E}\leftarrow w\p{E}/H\p{E}$. The reasoning behind this step is
such: say that for two energies $E_0$ and $E_b$ we have that
$H\p{E_0}=\bk{H}_E$ and $H\p{E_b}=b\bk{H}_E$, where $\bk{H}_E$ is the
mean histogram value over all energies. In this case, we wish to
decrease the current simulation bias on $E_b$ by a factor of $b$ while
keeping the bias on $E_0$ the same. As the weights $w\p{E}$ are
proportional to the simulation bias on each respective energy $E$,
dividing the weights $w\p{E}$ by the current histogram $H\p{E}$
achieves the desired changes to the simulation biases. The factor
$\bk{H}_E$ in this weight update has no effect on the function of the
weights, and is in practice immediately scaled out of the weight
array.

In our implementation of the simple flat method, we used $u=2$ and
$n_0=L\exp\p{\epsilon/kT_{\t{min}}}$, where $L$ is the number of
energy levels of the given square-well fluid and $T_{\t{min}}$ is a
minimum temperature of interest for the current simulation. The factor
of $L$ appears because initialization time should scale with the range
of energies of the simulated system. If we have twice the number of
energies, we should explore energy space for twice as long. To
estimate $L$, we multiplied the maximum number of spheres $M$ which
fit within a radius of $\lambda\sigma$ of a single sphere on a
face-centered cubic lattice (i.e. the maximum number of spheres with
which a given sphere could interact) by the total number of spheres
$N$, and divided the result by two so as to not double-count
interactions between spheres (so $L=MN/2$). The exponential
$\exp\p{\epsilon/kT_{\t{min}}}$, a Boltzmann factor for two adjacent
energies, appears because lower minimum temperatures of interest
warrant sampling lower energies, which in turn requires simulating for
longer (in step \ref{alg:simple_flat_sim} of Algorithm
\ref{alg:simple_flat}) because these energies have lower state
densities. The introduction of a minimum temperature may seem ad-hoc,
but all simulations in this paper require a choice of minimum
temperature anyways (discussed in Section \ref{sec:min_energy}). In
this paper, we will always use $kT_{\t{min}}=0.2\epsilon$.

\subsection{The Wang-Landau (WL) method}
\label{sec:wang_landau}

The Wang-Landau method is the first published histogram method we
discuss\cite{wang_landau, wang_landau_mod, wang_landau_analysis}. This
method has been used to study a wide variety of systems\fixme{which
  systems? cite}, and is the standard method\fixme{cite} to achieve
broad energy sampling via Monte Carlo simulations. \fixme{more intro
  on WL}

Unlike the simple flat method, the Wang-Landau method modifies the
weight array on the fly. After every move during initialization
Wang-Landau decreases the weight $w\p{E}$ on the current energy $E$ by
some factor of $f$, so as to decrease weights on the most commonly
observed energies relative to those of the least commonly observed
energies. Eventually, such a simulation with constant modification of
the weights $w\p{E}$ should yield a flat histogram
$H\p{E}\approx\bk{H}_E$ for all $E$, as the modifications to $w\p{E}$
increasingly pushes the simulated system away from the most sampled
energies, and toward the least sampled energies. When the energy
histogram is sufficiently flat, $H\p{E}$ is reset (i.e. set to
$H\p{E}=0$ for all $E$), the factor $f$ is decreased (geometrically
approaching unity), and the entire process is repeated until $f-1$
falls below some cutoff $c\ll 1$.

Resetting and repeating this initialization process is necessary to
perform fine tuning and refinement of the weights. A flat histogram
after a simulation in which $w\p{E}$ has changed does not guarantee
that simulating with the final weights will likewise result in a flat
histogram. Eventually, when $f-1<c\ll 1$, i.e. $f\approx 1$, the
initialization process no longer makes any appreciable modifications
to the weights $w\p{E}$, so that upon achieving a flat histogram one
can be confident that a regular simulation which fixes the current
weights will yield similar results.

\begin{algorithm}[tb]
  \caption{Wang-Landau initialization of weights}
  \label{alg:wang_landau}
  \begin{alg}

  \item Construct an initial typical fluid configuration

  \item Set $k=0$, choose some factor $f_0>1$, and initialize
    $w\p{E}=1$ for all $E$.

  \item Simulate for $n$ iterations. After each move, increment the
    histogram $H\p{E}$ of energy observations and update
    $w\p{E}\leftarrow w\p{E}/f_k$, where $E$ is the current energy of
    the system.
    \label{alg:wang_landau_sim}

  \item If the histogram $H\p{E}$ is not sufficiently flat, i.e. if it
    fails a ``flatness'' condition $C\sp{H}$, return to step
    \ref{alg:wang_landau_sim}, otherwise
    \begin{enumerate}
    \item reset $H\p{E}=0$ for all $E$,
    \item increment $k\leftarrow k+1$,
    \item set $f_k=\sqrt[u]{f_{k-1}}$ for some predetermined $u>1$,
      \label{alg:wang_landau_update}
    \item if $f_k\ge c$ for some predetermined $c$, return to step
      \ref{alg:wang_landau_sim}.
      \label{alg:wang_landau_end}
    \end{enumerate}

  \end{alg}
\end{algorithm}

Algorithm \ref{alg:wang_landau} spells out the Wang-Landau
method. This algorithm includes four free parameters which affect
initialization process: the number of iterations $n$ to run in step
\ref{alg:wang_landau_sim}, the initial factor $f_0$, the update factor
$u$, and the flatness condition $C\sp{H}$. The cutoff $c$ determines
the condition for Algorithm \ref{alg:wang_landau} to terminate.

The first of these parameters, $n$, controls how often to check the
flatness condition $C\sp{H}$ and run updates, if necessary. The value
of $n$ is not too important, so long as it is reasonable for the
system at hand. Too small value of $n$ will waste computation time
determining $C\sp{H}$, while too large of a value will waste time
simulating when $C\sp{H}$ has already been satisfied. In general, $n$
should be proportional to the computation time of $C\sp{H}$, which
scales with energy range of the simulated system. We therefore use
$n=L$, where $L$ is, as described in Section \ref{sec:simple_flat},
the number of energy levels for the given square-well fluid. Other
works typically neglect providing any heuristic or formula for $n$,
reporting a system-independent $n=10^3$\cite{wang_landau_mod} or
$10^5$\cite{wang_landau}.  \fixme{find more citations for this last
  claim}

The next parameter, $f_0$, controls the initial amount by which to
adjust the weight of the current energy after each move. As mentioned
in Algorithm \ref{alg:wang_landau}, one should always have $f_0>1$,
but the appropriate value of $f_0$ will generally depend on the system
at hand. Too large a value of $f_0$ will magnify stochastic error in
initialization, while too small a value of $f_0$ will cause
simulations to take an exceedingly long time to converge. Lacking any
heuristics for assigning $f_0$, literature typically suggests $f_0=e$
\cite{wang_landau}. After some experimentation with the free
parameters in the Wang-Landau method, we decided on $f_0=e^{1/8}$. In
practice, one usually works with $\ln f$, making the commonly
suggested value $\ln f_0=1$ and our own $\ln f_0=1/8$.

The factor $u$ appearing in step \ref{alg:wang_landau_update} of
Algorithm \ref{alg:wang_landau} controls the amount by which to adjust
$f$ when the histogram $H\p{E}$ is sufficiently flat. In principle,
one need not take a root of $f$ at each reset: the general idea behind
Algorithm \ref{alg:wang_landau} requires only $f_k<f_{k-1}$ to
converge (assuming that $C\sp{H}$ can be satisfied) and
$\lim_{k\to\infty}f_k=1$ to terminate. Taking the $u$-th root of $f$
at each increment of $k$ is a natural and convenient iterative means
to satisfy these requirements. Previous works have reported using
values such as $u=2$ \cite{wang_landau, wang_landau_mod} and $u=10$
\cite{wang_landau_analysis}; we use the former, conservatively
favoring more initialization iterations by decreasing $f$ by a smaller
amount on each reset.

One might guess why the flatness condition $C\sp{H}$ for updating $f$
and restarting the Wang-Landau initialization process should not be
made too lax: doing so would prompt the initialization process to
\begin{enumerate*}[label=\roman*)]
\item reset and decrease $f$ too early, i.e. when the weight array
  could still use some heavy handed adjustment, and
\item quit before one has any reason to think that simulations might
  yield a flat histogram.
\end{enumerate*}
Less obvious, however, is the fact that the flatness condition
$C\sp{H}$ should not be made too stringent: if the histogram is
completely flat, say $H\p{E}=H_0$ for all $E$, then the weights
$w\p{E}$ have all been modified by the same factor of $f^{H_0}$.
Remembering the scale-invariance of $w\p{E}$, this result would mean
that the weights have not actually changed at all!

The success of Wang-Landau thus relies on some degree of leniency in
the flatness condition, which makes tuning $C\sp{H}$ crucial to the
efficient and effective implementation of this method. Some previous
works\cite{wang_landau} suggest that $C\sp{H}$ should be made as
stringent as possible, cautioning only that some simulations might not
satisfy too strict of a flatness condition in any reasonable amount of
time. These works fail to recognize the reliance of Wang-Landau on the
{\it failure} of $C\sp{H}$ to enforce a perfectly flat histogram. A
commonly suggested flatness condition is that the minimum histogram
value in some specified range of energies be at least some fixed
proportion of the mean histogram value, i.e.
$C\sp{H}:\min\sp{H\p{E}}\ge x\bk{H}_E$, and several papers report
using $x=0.95$. We used the same condition with $x=0.25$. Determining
the range of energies over which to evaluate $C\sp{H}$ is the subject
of Section \ref{sec:min_energy}.

The final free parameter in the Wang-Landau method is the cutoff $c$,
which determines how small $f$ can get before the algorithm stops
modifying the weight array. Too large of a value for $c$ will make
Wang-Landau quit prematurely, whereas too small a value will cause the
algorithm to waste time simulating after the weights $w\p{E}$ have
essentially converged. One might imagine using an alternate end
condition for Wang-Landau, which involves comparing the current
weights $w_k\p{E}$ to those at the end of the previous cycle,
$w_{k-1}\p{E}$, to check whether the initialization process is still
making appreciable modifications to the weights, but such a check
would require both a metric and a free parameter to define what an
appreciable change is. For simplicity, we stuck with the standard end
condition given in Algorithm \ref{alg:wang_landau}. As with the other
free parameters in Wang-Landau, we are not aware of any heuristics for
determining an appropriate value of $c$, but literature cites typical
values of $\ln c=10^{-8}$. In order to compare different histogram
methods fairly, we used an alternate end condition in step
\ref{alg:wang_landau_end} of Algorithm \ref{alg:wang_landau}
(discussed in Section \ref{sec:end_conditions}), and therefore did not
need to choose a value of $c$.

\subsection{The transition matrix Monte Carlo (TMMC) method}
\label{sec:tmmc}

Unlike the previous histogram methods, the transition matrix Monte
Carlo method does not use weights, and merely compute the density of
states $D\p{E}$ of a system\cite{tmmc}. The density of states can in
turn be used to determine flat histogram weights via
(\ref{eq:flat_weights}). TMMC introduces a new object: the energy
transition matrix, $T$, whose components $T_{ij}$ are the
probabilities that a system will transition {\it to} a state with
energy $E_i$ {\it from} a given state with energy $E_j$,
i.e. $T_{ij}=P\p{E_j\to E_i}$.

The transition matrix is trivially square and positive-definite, but
it is not symmetric. Given that Metropolis Monte Carlo samples all of
state-space randomly, meaning all possible system states are equally
likely to occur during simulation, a transition $E_i\to E_f$ from an
energy with a low state density to one with a high state density,
$D\p{E_i}<D\p{E_j}$, is more probable than the inverse transition
$E_j\to E_i$ simply due to the fact that there are more states with
energy $E_i$ than those with energy $E_j$. Thus $D\p{E_i}<D\p{E_j}$
implies $P\p{E_i\to E_j}>P\p{E_j\to E_i}$, which means
$T_{ji}>T_{ij}$. The asymmetry of the transition matrix is precisely
the origin of the second law of thermodynamics: systems are
exceedingly likely to evolve towards macroscopic states with higher
accessible microstate densities.

Knowing the transition matrix of a system determines, among other
properties, the system's density of states. Consider an ensemble of
Metropolis Monte Carlo simulations with a distribution of energies at
a time $t$ given by $\hat D^{t}\p{E}$, such that the probability of
any given simulation to have an energy $E_i$ at a time $t$ is $\hat
D_i^{t}=\hat D^{t}\p{E_i}$. The probability $\hat D_i^{t}$ can be
expressed in terms of the distribution $\hat D^{t-1}\p{E}$ one time
step (i.e. one Metropolis Monte Carlo move) prior by
\begin{align}
  \hat D_i^{t}=\sum_jP\p{E_i\to E_j}\hat D_j^{t-1} =\sum_j T_{ij}\hat
  D_j^{t-1}.
  \label{eq:transition_evolution}
\end{align}
At equilibrium, the probability distributions do not vary with time,
meaning $\hat D^{t}=\hat D^{t-1}$. Furthermore, the distribution of
states in an equilibrium ensemble of Metropolis Monte Carlo
simulations is precisely the density of states $D\p{E}$, which means
$D_i=T_{ij}D_j$.  Simultaneously expressing this condition for all
energies,
\begin{align}
  D=TD. \label{eq:dos_eigen}
\end{align}
Finding the density of states of a system can thus be reduced to
finding the eigenvector of the transition matrix which has a
corresponding unit eigenvalue\fixme{why is this eigenvector
  unique?}. Once one has determined the density of states of a system,
one can initialize weights via the flat histogram method,
i.e. (\ref{eq:flat_weights}).

Computationally determining the transition matrix of a system is
straightforward: one need only simulate the system and collect a
histogram $\tilde T_{ij}=\tilde T\p{E_i,E_j}$ of the energy
transitions after each Metropolis Monte Carlo move, i.e. after step
\ref{alg:metropolis_move} of Algorithm \ref{alg:metropolis}. A
histogram of transitions $\tilde T\p{E_i,\bar E_j}$ from a fixed
energy $\bar E_j$ to energies $E_i$ is then proportional to the
probability distribution of transitions from $\bar E_j$, i.e. the
transition matrix $T\p{E_i,\bar E_j}$. Crucially, this proportionality
does not depend on the history of a simulation, or on how the
simulation got to be in the energy $\bar E_j$, so long as samples of
the energy $\bar E_j$ are themselves void of systemic
bias. Furthermore, one does not actually need to transition from $E_j$
to $E_i$ in order to collect statistics on $T\p{E_i,E_j}$: after
deciding whether to accept a move based on whether it results in a
valid system state, one can increment $\tilde T\p{E_i,E_j}$ before
deciding to reject the move for some other reason, e.g. because
$w\p{E_j}\gg w\p{E_i}$, or even because one wishes to collect more
statistics on $T\p{E_i,\bar E_j}$.

The proportionality $\tilde T_{ij}\propto T_{ij}$ can be made explicit
by enforcing that the probabilities of all transitions from an energy
$E_j$ sum to unity, i.e.
\begin{align}
  \sum_iP\p{E_j\to E_i}=\sum_iT_{ij}=1,
  \label{eq:transition_norm_condition}
\end{align}
which implies
\begin{align}
  T_{ij}=\f{\tilde T_{ij}}{\sum_i\tilde T_{ij}}.
  \label{eq:transition_normalization}
\end{align}
Computing $T$ from $\tilde T$ therefore requires normalizing each row
of $\tilde T$ independently.

If a system has $L$ energy levels, the transition histogram $\tilde
T\p{E_i,E_j}$ is an $L\times L$ matrix. Given that $L$ is proportional
to the system size $N$, the memory footprint of $\tilde T\p{E_i,E_j}$
grows as $N^2$. For most systems, however, there will be a maximum
energy $M$ independent of system size by which a single move can
change the energy of the system. The transition histogram for such
systems is therefore band-diagonal, which one can exploit to save
memory. In particular, it is natural to store the transition histogram
in the form $\tilde T_d\p{E,\Delta E}$, which gives the probability
that a system will transition from the energy $E$ to the energy
$E+\Delta E$ in a single move. One can convert between $\tilde
T_d\p{E,\Delta E}$ and $\tilde T\p{E_i,E_j}$ using the relation
\begin{align}
  \tilde T\p{E_i,E_j}=\tilde T_d\p{E_j,E_i-E_i},
\end{align}
which means, from (\ref{eq:transition_normalization}),
\begin{align}
  T_{ij}=\f{\tilde T_d\p{E_j,E_i-E_j}}{\sum_i\tilde
    T_d\p{E_j,E_i-E_j}} =\f{\tilde T_d\p{E_j,E_i-E_j}} {\sum_{\Delta
      E}\tilde T_d\p{E_j,\Delta E}}.
  \label{eq:transition_conversion}
\end{align}
Expressed in this form, the memory footprint of the transition
histogram grows linearly with system size $N$ and the maximal energy
difference $M$.

\begin{algorithm}[tb]
  \caption{Transition matrix Monte Carlo initialization}
  \label{alg:tmmc}
  \begin{alg}

  \item Construct an initial typical fluid configuration.

  \item Initialize a transition histogram $\tilde T_d\p{E,\Delta E}=0$
    for all $E$ and $\Delta E\in\sp{-M,M}$, where $M$ is the possible
    maximum energy transition of a single move.

  \item Randomly attempt to change the position of one sphere,
    tentatively accepting the transition from state $s_i$ to state
    $s_f$ if $s_f$ is not a forbidden fluid configuration.
    \label{alg:tmmc_move}

  \item Let $\Delta E=E_f-E_i$ and increment $\tilde
    T_d\p{E_i,\Delta{E}}$, where $E_k$ is the energy of state $s_k$.

  \item If $\Delta E<0$, accept the move. Otherwise
    \begin{enumerate}
    \item compute the normalization factor
      \begin{align*}
        n_k=\sum_{\Delta E'}\tilde T_d\p{E_k,\Delta E'}
      \end{align*}
      for $k\in\set{i,f}$, and

    \item accept the move $s_i\to s_f$ with probability
      \begin{align*}
        P_m\p{s_i\to s_f} =\max\set{\f{\p{\tilde T_d\p{E_i,\Delta
                E}+c}/\p{n_i+c}} {\p{\tilde T_d\p{E_f,-\Delta
                E}+c}/\p{n_f+c}}, \exp\p{-\f{\Delta E}{kT_{\t{min}}}}}
      \end{align*}
      for a predetermined number $c$ and temperature $T_{\t{min}}$.
      \label{alg:tmmc_prob}
    \end{enumerate}

  \item If some predetermined initialization end condition has
    \emph{not} been met, return to step
    \ref{alg:tmmc_move}. Otherwise, compute the density of states
    $D\p{E}$ by solving (\ref{eq:dos_eigen}), and set weights
    $w\p{E}=1/D\p{E}$.

  \end{alg}
\end{algorithm}

Figure \ref{fig:transitions_sample} provides an example of a
transition matrix $T_d\p{E,\Delta E}$ for a particular square-well
fluid. The band at $\Delta E=0$ indicates that most moves transition
from an energy to itself. Low energies are more likely to transition
up in energy, with $\Delta E>0$, than down in energy, with $\Delta
E<0$. The high energies which are more likely to transition down in
energy than up are those above the state of maximum entropy.

\begin{figure}
  \centering
  \includegraphics[width=0.6\textwidth]{figs/transitions-example.pdf}
  \caption[Transition matrix]
  {Sample transition matrix $T_d\p{E,\Delta E}$ for square-well fluid
    with a well width $\lambda=1.3$, filling fraction $\eta=0.3$, and
    $N=25$ spheres.  The band at $\Delta E=0$ indicates that any given
    state is most likely to transition into another state with the
    same energy.}
  \label{fig:transitions_sample}
\end{figure}

While one could imagine many initialization routines to determine the
transition matrix $T$, we use Algorithm \ref{alg:tmmc}, which is a
modified version of a routine provided in \cite{tmmc_prl}\fixme{is
  this citation correct?}. The primary motivation behind this
algorithm is such: as lower energies are always more difficult to
sample than higher energies, transitions to lower energies should
always be accepted, while transitions to higher energies should pass a
probabilistic acceptance test. To understand the probability in step
\ref{alg:tmmc_prob} of Algorithm \ref{alg:tmmc} of accepting a move
$s_i\to s_f$ for which $E_f>E_i$, first consider the case of $c=0$ and
$T_{\t{min}}\to0$, in which case the probability becomes
\begin{align}
  P_m\p{s_i\to s_f}=\f{\p{\tilde T_d\p{E_i,\Delta E}}\big/
    \p{\sum_{\Delta E'}\tilde T_d\p{E_i,\Delta E'}}} {\p{\tilde
      T_d\p{E_f,-\Delta E}}\big/ \p{\sum_{\Delta E'}\tilde
      T_d\p{E_f,\Delta E'}}} =\f{T_d\p{E_i,\Delta
      E}}{T_d\p{E_f,-\Delta E}} =\f{P\p{E_i\to E_f}}{P\p{E_f\to E_i}},
  \label{eq:tmmc_prob_simplified}
\end{align}
where $\Delta E=E_f-E_i$ and $P\p{E_m\to E_n}$ is the unbiased
probability of a transition from an energy $E_m$ to an energy
$E_n$. The result in (\ref{eq:tmmc_prob_simplified}) is the
probability of accepting moves $s_i\to s_f$ required for simulation
via Algorithm \ref{alg:tmmc} to yield a flat energy
histogram\cite{tmmc}. Unlike the previous histogram methods, we desire
a flat histogram during initialization via Algorithm \ref{alg:tmmc}
not because we are constructing weights $w\p{E}$, but because we wish
to sample the transition histogram $\tilde T_d\p{E,\Delta E}$ equally
at all energies $E$. \fixme{briefly explain
  (\ref{eq:tmmc_prob_simplified}) by considering a two-level system,
  and extending to larger systems}.

Aside from a minimum temperature $T_{\t{min}}$ and an end condition,
Algorithm \ref{alg:tmmc} contains one free parameter, $c$, used in
step \ref{alg:tmmc_prob} to modify (\ref{eq:tmmc_prob_simplified}).
This parameter makes the rejection of transitions more conservative
when there are too few statistics in the histogram $\tilde T_d$ to be
confident in the current estimate of the transition matrix. In order
to make the effects of $c$ negligible as $\tilde T_d$ collects more
statistics throughout initialization, $c$ should be of order unity,
though its optimal value is generally system-dependent. In our
implementation of Algorithm \ref{alg:tmmc}, we use $c=16$.

The last interesting part of the probability in step
\ref{alg:tmmc_prob} of Algorithm \ref{alg:tmmc} is the limit on
$P_m\p{s_i\to s_f}$ to a minimum value of $\exp\p{-\Delta
  E/kT_{\t{min}}}$. This limit caps the bias on energies to that
introduced by canonical (fixed-temperature) weights at a temperature
of $T_{\t{min}}$. As a result, Algorithm \ref{alg:tmmc} will not waste
time oversampling energies which are unimportant for determining
system properties at temperatures $T\ge T_{\t{min}}$.

\subsection{The optimized ensemble (OE) method}
\label{sec:optimized_ensemble}

All histogram methods introduced in this paper thus far have focused
on determining a weight array $w\p{E}$ for a flat energy histogram
$H\p{E}$, in order to sample and collect statistics on all energies
equally. This motivation considers the quantity of statistics, but not
their quality. Biased simulations collect statistics on low energy
states by first getting into these states, and then rejecting moves
which move the system into higher energy states. Statistics on low
energy states will therefore generally be highly correlated, as they
are based on many samples of only a few low energy regions of a
system's energy landscape. The optimized
ensemble\cite{optimized_ensemble} attempts to address the
autocorrelation between low energy samples by finding weights to
maximize the rate at which a simulation makes round trips between low
and high energy states. Maximizing the round trip rate, in turn,
maximizes the rate at which a simulation makes independent,
uncorrelated samples of low energies.

The optimized ensemble works by considering an ensemble of
simulations, each of which defines the position of a ``walker'' in
energy space. Given that we want walkers to move back and forth
between two extrema $E_+$ and $E_-$ of some specified energy range, we
label each walker by which extrema it has visited most recently. We
can then identify ``down-going'' (``up-going'') walkers by those which
more recently visited $E_+$ ($E_-$), which means that to make a round
trip they first needed to get to $E_-$ ($E_+$). Denoting the total
walker and down-going density at an energy $E$ respectively by
$n\p{E}$ and $n_+\p{E}$, the ratio $f\p{E}=n_+\p{E}/n\p{E}$ gives the
proportion of walkers at $E$ which are down-going. We also denote the
walker diffusivity at an energy $E$ by $\alpha\p{E}$.

At equilibrium with a flat histogram, meaning $n$ is constant, the
down-going walker current $j\p{E}$ can be expressed in terms of the
diffusivity $\alpha\p{E}$ by
\begin{align}
  j=-\alpha\f{dn_+}{dE}.
  \label{eq:walker_current_definition}
\end{align}
This identity can be taken as the definition of the walker diffusivity
$\alpha$, which is the independent of the label we have assigned any
given walker. Substituting $n_+=fn$,
\begin{align}
  j=-\alpha\p{n\f{df}{dE}+f\f{dn}{dE}}=-\alpha n\f{df}{dE},
  \label{eq:walker_current}
\end{align}
where a constant $n$ implies $dn/dE=0$. We can rearrange
(\ref{eq:walker_current}) and integrate over the energy range as
\begin{align}
  \int_{f=f\p{E_-}}^{f\p{E_+}}\f{df}{j}=-\int_{E=E_-}^{E_+}\f{dE}{\alpha
    n}.
  \label{eq:walker_current_integral_setup}
\end{align}
At equilibrium, the number of down-going walkers $n_+\p{E}$ at any
given energy $E$ is constant with time, which means that the walker
flux $j\p{E}$ must be constant with energy. We can therefore evaluate
one of the integrals in (\ref{eq:walker_current_integral_setup}) to
get
\begin{align}
  \f1{\abs j}=\int_{E=E_-}^{E_+}\f{dE}{\alpha n}.
  \label{eq:walker_current_integral}
\end{align}

The optimized ensemble method minimizes the integral in
(\ref{eq:walker_current_integral}), thereby maximizing the down-going
walker current $\abs{j}$, by varying the walker density $n$ with the
constraint that $n$ must remain normalized. With the additional
assumption that $\alpha\p{E}$ does not strongly depend on the weights
$w\p{E}$, the authors of \cite{optimized_ensemble} find that the
optimal walker density $n$ is
\begin{align}
  n_{\t{opt}}\propto\f1{\sqrt{\alpha}},
  \label{eq:optimal_walker_density}
\end{align}
where the proportionality is determined by enforcing that $n$ is
normalized.

Equating the set of observations in a single simulation with a single
observation of an ensemble of simulations, the walker density $n\p{E}$
is proportional to the energy histogram $H\p{E}=\tilde D\p{E}w\p{E}$,
which means
\begin{align}
  w_{\t{opt}}=\f1{D\sqrt{\alpha}},
  \label{eq:optimized_weights}
\end{align}
where we neglect the distinction between $\tilde D$ and $D$, which has
no effect on the weights $w$.

The algorithm provided in \cite{optimized_ensemble} to construct
weights given by (\ref{eq:optimized_weights}) finds the diffusivity
$\alpha$ by solving for it in
(\ref{eq:walker_current_definition}). This procedure accepts flat
histogram weights, or equivalently the density of states, as an input,
and is not meant to construct weights from scratch. We therefore will
not be comparing the optimized ensemble to other methods in this
paper. We do, however, credit \cite{optimized_ensemble} for motivating
the hybrid OE-TMMC method in Section \ref{sec:oetmmc}.

\subsection{The hybrid OE-TMMC method}
\label{sec:oetmmc}

As well as the density of states, the transition matrix $T\p{E_i,E_j}$
determines the local diffusivity $\alpha\p{E}$ for any given weights
$w\p{E}$. Denoting by $P_b\p{E_i\to E_j}$ the actual, biased
probability of transitioning from $E_i$ to $E_j$ in a single Monte
Carlo move, we can say that
\begin{align}
  P_b\p{E_i\to E_f}=P\p{E_i\to E_f}P_m\p{E_i\to E_f}
  =T_{ji}\max\set{\f{w\p{E_f}}{w\p{E_i}},1},
\end{align}
where $P\p{E_i\to E_j}$ is the probability of attempting the move
$E_i\to E_j$ and $P_m\p{E_i\to E_j}$ is the probability of accepting
that move, should it occur. In terms of a transition histogram $\tilde
T\p{E,\Delta E}$ with $E=E_i$ and $\Delta E=E_f-E_i$,
\begin{align}
  P_b\p{E\to E+\Delta E} =\f{\tilde T_d\p{E,\Delta E}w\p{E+\Delta
      E}/w\p{E}} {\sum_{\Delta E'}\tilde T_d\p{E,\Delta
      E'}w\p{E+\Delta E'}/w\p{E}}.
  \label{eq:actual_transition_prob}
\end{align}
We now define
\begin{align}
  \bk{\Delta E}_E=\sum_{\Delta E}\Delta E P_b\p{E\to E+\Delta E}
  =\f{\sum_{\Delta E}\Delta E~\tilde T_d\p{E,\Delta E}
    \max\set{w\p{E+\Delta E}/w\p{E},1}} {\sum_{\Delta E'}\tilde
    T_d\p{E,\Delta E'} \max\set{w\p{E+\Delta E'}/w\p{E},1}}
  \label{eq:<de>}
\end{align}
and
\begin{align}
  \bk{\Delta E^2}_E=\sum_{\Delta E}\Delta E^2 P_b\p{E\to E+\Delta E}
  =\f{\sum_{\Delta E}\Delta E~\tilde T_d\p{E,\Delta E}
    \max\set{w\p{E+\Delta E}/w\p{E},1}} {\sum_{\Delta E'}\tilde
    T_d\p{E,\Delta E'} \max\set{w\p{E+\Delta E'}/w\p{E},1}},
  \label{eq:<de^2>}
\end{align}
in terms of which the diffusivity $\alpha\p{E}$ is
\begin{align}
  \alpha\p{E}=\bk{\Delta E^2}_E -\bk{\Delta E}_E^2.
  \label{eq:microscopic_diffusitivy}
\end{align}
Diffusivity $\alpha\p{E}$ is thus the variance of the change in energy
$\Delta E$ from as single move out of $E$. Reconciling this definition
with that we provide in (\ref{eq:walker_current_definition}) is
considered outside the scope of this paper.

The hybrid OE-TMMC method thus first runs Algorithm \ref{alg:tmmc} to
collect a transition histogram and compute flat histogram weights,
then computes the diffusivity $\alpha\p{E}$ via
(\ref{eq:<de>})-(\ref{eq:microscopic_diffusitivy}), and finally
computes optimized weights as given by
(\ref{eq:optimized_weights}). Unlike the optimized ensemble, OE-TMMC
does not require any extra simulation routines, and is therefore
straightforward to implement and compare to other histogram methods.
\fixme{provide iterative algorithm?}

\subsection{Loose ends}
\label{sec:loose_ends}

Throughout the discussion and development of histogram methods, we
have thus far neglected a few important subjects:
\begin{enumerate*}
\item initialization end conditions i.e. the means by which we decide
  when an algorithm to construct weights is finished,
\item the minimum temperature $T_{\t{min}}$ which we have sometimes
  mentioned,
\item the energy range over which we are interested in, and
\item what to do with the weight array outside of the energy range of
  interest, or at in which it is not defined.
\end{enumerate*}
The last three of these subjects turn out to be related, as the
minimum temperature $T_{\t{min}}$ chosen for any given simulation
determines the minimum energy $E_-$ of interest, as well as the
weights $w\p{E}$ at energies $E<E_-$.

\subsubsection{End conditions}
\label{sec:end_conditions}

While the Wang-Landau method comes equipped with an end condition of
its own, most histogram methods require some external means of
deciding when they are finished initializing. Wang-Landau's
``natural'' end condition does not advantage it over other methods,
however, as the WL end condition makes use of a free parameter, which
any end condition will require. In order to compare all methods
fairly, we need a method-independent end condition. The most
straightforward of such end conditions is to simply to stop
initializing after some fixed number of Monte Carlo iterations, and
appropriately finish up any other calculations in the algorithm before
terminating it. We thus initialize systems of $N$ spheres for
$10^5N^2$ iterations; the choice of $10^5N^2$ we will address in
Section \ref{sec:results}\fixme{address in results section}. In
general, however, one might want an end condition which is independent
of system size. We came up with a four such end conditions as
examples, though we do not use these end conditions in this paper.

Let $E_+$ be the energy at which the density of states $D\p{E}$ and
entropy $S\p{E}$ are maximal, and let $E_-$ be the minimum energy of
interest. We define the pessimistic sample count $s_p\p{E_0}$ as the
number of times which a system has traveled from $E_+$ to $E_0$, in
any number of moves. This count represents the most pessimistic
estimate of the number of independent, uncorrelated observations of
the energy $E_0$, as going to the state of maximal entropy between
samples wipes all correlations between them. We similarly define an
optimistic sample count $s_o\p{E_0}$ as the number of times which the
system has transitioned into $E_0$ from any energy $E>E_0$. This count
represents the most optimistic estimate of the number of independent
observations of the energy $E_0$, as the system had merely to go to
any energy $E>E_0$, where the density of states may be only marginally
larger, between each sample.

In general, any initialization routine should sample the entire range
$R=\sp{E_-,E_+}$ of interest for simulation before terminating.
Furthermore, samples at the end of initialization should be numerous
enough to be statistically significant. Given that any samples of
$E_-$ necessarily require first sampling all other energies in $R$ on
the way down from $E_+$, this energy ($E_-$) will generally have the
fewest number of samples. As an end condition, therefore, one can
simply enforce some minimum number of optimistic or pessimistic
samples at $E_-$, i.e. $s_o\p{E_-}$ or $s_p\p{E_-}$. A more
sophisticated end condition would be to choose a minimum temperature
of interest $T_{\t{min}}$ and initialize until the mean fractional
error $\bk{\delta_s}_{E,T_{\t{tmin}}}$ in either of the sample counts
$s_o\p{E}$ or $s_p\p{E}$ reaches some minimum value. These mean errors
are
\begin{align}
  \bk{\delta_s}_{E,T}=\bk{s^{-1/2}}_{E,T}=\sum_Es\p{E}^{-1/2}P\p{E,T}
  =\f{\sum_Es\p{E}^{-1/2}D\p{E}e^{-E/kT}}{\sum_{E'}D\p{E'}e^{-E'/kT}},
  \label{eq:fractional_sample_error}
\end{align}
where we use the fact that the fractional error $\delta_G$ in any
histogram $G$ collected via random Monte Carlo sampling is $G^{-1/2}$.

\subsubsection{The energy range of interest}
\label{sec:min_energy}

Using histogram methods generally requires knowing the energy range of
interest for the system at hand. The Wang-Landau method, for example,
periodically checks a flatness condition $C\sp{H}$ on the energy
histogram $H\p{E}$. In order for such a condition to be well-defined,
it needs have a specified energy range over which to check the
flatness of $H$. While some systems, such as the Ising model, have
well-defined and easily computable energy range, the same is not true
of most systems. In the case of the square-well fluid, not only is the
minimum energy unknown, some energies have such an incredibly low
state density $D\p{E}$ that one cannot reasonably expect to ever
observe them via Monte Carlo, biased or otherwise. We therefore need
to identify an energy range of interest, specified by a maximum and
minimum energies of interest.

Identifying a maximum energy of interest is straightforward: this is
simply the energy $E_+$ at which the density of states $D\p{E}$ is
maximal. Though high energies become more important for determining
system properties $\bk{X}_T$ at higher temperatures, the relative
importance of energies $E>E_+$ to the energy $E_+$ approaches a
maximum as $T\to\infty$. There is therefore a maximum weight $w\p{E}$
which is appropriate to assign energies $E>E_+$ relative to the weight
$w\p{E_+}$, which is namely the ratio of these weights as
$T\to\infty$. As observed in (\ref{eq:inf_temp_weights}), the weight
array becomes flat as $T\to\infty$, which means that the ratio of
$w\p{E>E_+}$ to $w\p{E}$ appropriate for any given temperature $T$
maximally approaches unity. We can therefore always set
$w\p{E>E_+}=w\p{E_+}$, which will ensure that energies $E>E_+$ are not
sampled any more than is necessary for any positive temperature. The
maximum of $D\p{E}$ can be found by simulating without weights for a
short while, after which the energy histogram $H\p{E}$ should be
maximal at $E_+$.

The minimum important energy $E_-$ for a system is less obvious than
$E_+$. Just as finding $E_+$ naturally required using a ``maximal''
temperature $T\to\infty$, determining $E_-$ requires a choice of some
minimum temperature $T_{\t{min}}$. The minimum important energy $E_-$
is then the energy at which the partition function terms
$D\p{E}e^{-E/kT_{\t{min}}}$ are maximal. Lower energy terms grow
increasingly less important with decreasing energy, and are
sufficiently sampled with canonical weights
$w\p{E<E_-}=e^{-E/kT_{\t{min}}}$ to determine any property
$\bk{X}_{T\ge T_{\t{min}}}$. To maximize $D\p{E}e^{-E/kT_{\t{min}}}$,
we need
\begin{align}
  \f{d}{dE}\p{De^{-E/kT_{\t{min}}}}
  =\f{dD}{dE}e^{-E/kT_{\t{min}}}+De^{-E/kT_{\t{min}}}\p{-\f{1}{kT_{\t{min}}}}=0,
  \label{eq:min_E_setup}
\end{align}
which means
\begin{align}
  \f1{D}\f{dD}{dE}=\f{d\ln D}{dE}=\f{1}{kT_{\t{min}}}.
  \label{eq:min_E_condition}
\end{align}
Though we do not always know the density of states $D\p{E}$, in
practice we compute the best available estimate $D\p{E}$ every time we
need to know $E_-$. If we do not find any energy satisfying
(\ref{eq:min_E_condition}), we use the minimum energy the simulation
has ever observed in place of $E_-$ to define our energy range of
interest.

Once we identify an energy $E_-$ satisfying (\ref{eq:min_E_condition})
in an initialization routine for any histogram method, we effectively
forbid transitions to energies $E<E_-$ by tentatively setting weights
$w\p{E<E_-}=e^{-\p{E-E_-}/kT}$ for some temperature $T\ll
T_{\t{min}}$. These weights prevent the initialization routine from
wasting time on energies at which we will override the weight array
after initialization anyways. A special case was made for Wang-Landau,
as the design of this method assumes knowledge of the energy range of
interest a priori. In order to fairly compare Wang-Landau to other
methods, we therefore identified $E_-$ for each simulated system using
other methods, and hard-coded the minimum important energies for each
system into the Wang-Landau initialization routines\fixme{talk about
  how bad this is}. We always set
$w\p{E<E_-}=e^{-\p{E-E_-}/kT_{\t{min}}}$ after initialization of all
histogram methods with $kT_{\t{min}}=0.2\epsilon$. We chose this
temperature because it is right below the temperature at which the
square-well fluid condenses from a gas into a liquid (see Section
\ref{sec:errors}). \fixme{what about pressure?}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results and Discussion}
\label{sec:results}

Having developed and implemented all of the histogram methods
discussed in Section \ref{sec:methods}, we will now look at the
results from simulating with these methods.

\subsection{Histograms and arrays}
\label{sec:histograms}

\fixme{Give examples of energy histograms for one system, density of
  states for several $N$, and perhaps a weight array.}

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{figs/array-example.pdf}
  \caption[Example arrays]
  {Example arrays computed by various histogram methods for a
    square-well fluid with a well width $\lambda=1.3$, filling
    fraction $\eta=0.3$, and $N=25$ spheres after simulating for
    $10^5N^2$ iterations: (top left) energy histogram, (top right)
    weight array, (bottom left) density of states.}
  \label{fig:arrays}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{figs/sample-rate-example.pdf}
  \caption[Example energy sampling rates]
  {Example pessimistic energy sampling rates computed by various
    histogram methods for a square-well fluid with a well width
    $\lambda=1.3$, filling fraction $\eta=0.3$, and $N=25$ spheres
    after simulating for $10^5N^2$ iterations.}
  \label{fig:sample_rates}
\end{figure}

\subsection{Thermodynamic properties and errors}
\label{sec:errors}

\fixme{Discuss errors in computed thermodynamic properties. Introduce
  converged flat weights.}

\begin{figure}[H]
  \centering
    \includegraphics[width=0.7\textwidth]{figs/internal-energy.pdf}
  \caption[Internal energy]
  {Specific internal energy}
  \label{fig:internal_energy}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\textwidth]{figs/heat-capacity.pdf}
  \caption[Heat capacity]
  {Specific heat capacity}
  \label{fig:heat_capacity}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{figs/config-entropy.pdf}
  \caption[Configurational entropy]
  {Specific configurational entropy}
  \label{fig:config_entropy}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]
  {figs/internal-energy-err.pdf}
  \caption[Errors in internal energy]
  {Errors in specific internal energy}
  \label{fig:internal_energy_err}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{figs/heat-capacity-err.pdf}
  \caption[Errors in heat capacity]
  {Errors in specific heat capacity}
  \label{fig:heat_capacity_err}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{figs/config-entropy-err.pdf}
  \caption[Errors in configurational entropy]
  {Errors in specific configurational entropy}
  \label{fig:config_entropy_err}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\textwidth]{figs/internal-energy-comps.pdf}
  \caption[Internal energy error comparisons]
  {Internal energy error comparisons}
  \label{fig:u_comps}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\textwidth]{figs/heat-capacity-comps.pdf}
  \caption[Heat capacity error comparisons]
  {Heat capacity error comparisons}
  \label{fig:u_comps}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\textwidth]{figs/config-entropy-comps.pdf}
  \caption[Configurational entropy error comparisons]
  {Configurational entropy error comparisons}
  \label{fig:u_comps}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}
\label{sec:conclusions}

\fixme{What have we learned? What recommendations can we make? When is
  it appropriate to use which histogram methods?}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\nocite{*} \bibliography{thesis}

\end{document}
